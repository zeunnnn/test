"""
LLM API ë¼ìš°í„° - LLMì´ ë„êµ¬ë¥¼ ììœ¨ì ìœ¼ë¡œ ì„ íƒ
"""
from fastapi import APIRouter
from pydantic import BaseModel
import httpx
import json
import re

router = APIRouter()

OLLAMA_API_URL = "http://localhost:11434/api/generate"

class ChatRequest(BaseModel):
    prompt: str

mcp_manager = None

def set_mcp_manager(manager):
    global mcp_manager
    mcp_manager = manager

SYSTEM_PROMPT_TEMPLATE = """/no_think
ë‹¹ì‹ ì€ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

{tools_description}

## ì‘ë‹µ ê·œì¹™
1. ë„êµ¬ê°€ í•„ìš”í•˜ë©´ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{"tool": "ë„êµ¬ì´ë¦„", "arguments": {{...}}}}

2. ë„êµ¬ê°€ í•„ìš”ì—†ìœ¼ë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë‹µë³€í•˜ì„¸ìš”.

3. ì˜ˆì‹œ:
- ê³„ì‚° ìš”ì²­: {{"tool": "add", "arguments": {{"input": {{"a": 5, "b": 3}}}}}}
- ìš´ì„¸ ìš”ì²­: {{"tool": "get_weekly_horoscope", "arguments": {{"sign": "virgo"}}}}

## ì£¼ì˜ì‚¬í•­
- JSON ì™¸ì˜ ì„¤ëª…ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ë„êµ¬ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ì˜¤ì§ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”
"""

async def call_ollama(prompt: str, system: str = None) -> str:
    """Ollama API í˜¸ì¶œ"""
    full_prompt = prompt
    if system:
        full_prompt = f"{system}\n\nì‚¬ìš©ì: {prompt}\n\nì–´ì‹œìŠ¤í„´íŠ¸:"
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        resp = await client.post(
            OLLAMA_API_URL,
            json={
                "model": "qwen3:4b",
                "prompt": full_prompt,
                "stream": False
            }
        )
        data = resp.json()
        return data.get("response", "")

def extract_json(text: str) -> dict | None:
    """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
    # ì½”ë“œ ë¸”ë¡ ë‚´ JSON
    patterns = [
        r'```json\s*(\{.*?\})\s*```',
        r'```\s*(\{.*?\})\s*```',
        r'(\{[^{}]*"tool"[^{}]*"arguments"[^{}]*\{.*?\}[^{}]*\})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                continue
    
    # ì „ì²´ê°€ JSONì¸ ê²½ìš°
    text = text.strip()
    if text.startswith("{"):
        try:
            # ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ ì°¾ê¸°
            brace_count = 0
            end_idx = 0
            for i, c in enumerate(text):
                if c == "{":
                    brace_count += 1
                elif c == "}":
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i + 1
                        break
            if end_idx > 0:
                return json.loads(text[:end_idx])
        except:
            pass
    
    return None

@router.post("/llm/")
async def chat_with_llm(request: ChatRequest):
    """MCP ë„êµ¬ë¥¼ í™œìš©í•œ LLM ì±„íŒ…"""
    global mcp_manager
    
    if not mcp_manager:
        return {"error": "MCP ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
    
    # ë„êµ¬ ì„¤ëª… ìƒì„±
    tools_desc = mcp_manager.get_tools_prompt()
    system = SYSTEM_PROMPT_TEMPLATE.format(tools_description=tools_desc)
    
    # 1ë‹¨ê³„: LLMì—ê²Œ ì§ˆë¬¸ (ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ê²°ì •)
    llm_response = await call_ollama(request.prompt, system)
    print(f"ğŸ“ LLM ì›ë³¸ ì‘ë‹µ: {llm_response[:200]}...")
    
    # 2ë‹¨ê³„: JSON ë„êµ¬ í˜¸ì¶œ íŒŒì‹±
    tool_call = extract_json(llm_response)
    
    if tool_call and "tool" in tool_call:
        tool_name = tool_call["tool"]
        arguments = tool_call.get("arguments", {})
        
        print(f"ğŸ”§ ë„êµ¬ í˜¸ì¶œ ê°ì§€: {tool_name}({arguments})")
        
        # 3ë‹¨ê³„: MCP ë„êµ¬ ì‹¤í–‰
        tool_result = await mcp_manager.call_tool(tool_name, arguments)
        print(f"ğŸ“¦ ë„êµ¬ ê²°ê³¼: {str(tool_result)[:200]}...")
        
        # 4ë‹¨ê³„: ê²°ê³¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±
        follow_up = f"""/no_think
ì‚¬ìš©ì ì§ˆë¬¸: {request.prompt}

ë„êµ¬ '{tool_name}' ì‹¤í–‰ ê²°ê³¼:
{json.dumps(tool_result, ensure_ascii=False, indent=2)}

ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ìš´ì„¸ì¸ ê²½ìš° í•µì‹¬ ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."""

        final_response = await call_ollama(follow_up)
        
        return {
            "response": final_response,
            "tool_used": tool_name,
            "tool_arguments": arguments,
            "tool_result": tool_result,
            "debug_llm_raw": llm_response
        }
    
    # ë„êµ¬ í˜¸ì¶œ ì—†ì´ ì¼ë°˜ ì‘ë‹µ
    return {
        "response": llm_response,
        "tool_used": None
    }

@router.get("/tools/")
async def list_tools():
    """ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ ëª©ë¡"""
    if not mcp_manager:
        return {"error": "MCP ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
    
    return {
        "tools": mcp_manager.get_tools_json_schema(),
        "prompt_format": mcp_manager.get_tools_prompt()
    }

@router.post("/llm/reload-tools/")
async def reload_tools():
    """MCP ë„êµ¬ ì¬íƒìƒ‰"""
    if not mcp_manager:
        return {"error": "MCP ë§¤ë‹ˆì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
    
    tools = await mcp_manager.discover_all_tools()
    return {"reloaded": len(tools), "tools": [t["name"] for t in tools]}